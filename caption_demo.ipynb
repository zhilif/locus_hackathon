{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e345c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad49d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import numpy as np\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528d85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ]) \n",
    "\n",
    "train_ds = datasets.CIFAR10('/home/zhilif/incremental_learn/FractalDB-Pretrained-ResNet-PyTorch/data', train=True, transform=preprocess, download=False)\n",
    "test_ds = datasets.CIFAR10('/home/zhilif/incremental_learn/FractalDB-Pretrained-ResNet-PyTorch/data', train=False, transform=preprocess, download=False)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=100,\n",
    "    shuffle=True, pin_memory=False)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=100,\n",
    "    shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98315f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 196 to 576\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\n"
     ]
    }
   ],
   "source": [
    "from models.blip import blip_decoder\n",
    "\n",
    "\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b220a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6180d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(trainLoader))\n",
    "# x = x.to(device)\n",
    "# y = y.to(device)\n",
    "# for i in range(20):\n",
    "#     with torch.no_grad():\n",
    "#         # beam search\n",
    "#         caption = model.generate(x[i:i+1], sample=False, num_beams=3, max_length=20, min_length=5) \n",
    "#         # nucleus sampling\n",
    "#         # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \n",
    "#         print('caption: '+ caption[0], trainLoader.dataset.classes[y[i].item()])\n",
    "#         plt.imshow(x[i].permute(1,2,0).cpu().numpy())\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8c4924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [10:52,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# mean_dict = dict()\n",
    "# count_dict = dict()\n",
    "# for i, (X, Y) in tqdm(enumerate(trainLoader)):\n",
    "#     for j in range(len(X)):\n",
    "#         if Y[j].item() not in mean_dict:\n",
    "#             mean_dict[Y[j].item()] = X[j]\n",
    "#             count_dict[Y[j].item()] = 1\n",
    "#         else:\n",
    "#             mean_dict[Y[j].item()] = mean_dict[Y[j].item()] * (count_dict[Y[j].item()] / (count_dict[Y[j].item()]+1)) + X[j] / (count_dict[Y[j].item()]+1)\n",
    "#             count_dict[Y[j].item()] += 1\n",
    "# #             print(Y[j], mean_dict[Y[j].item()].shape)\n",
    "\n",
    "mean_dict = dict()\n",
    "count_dict = dict()\n",
    "for i, (X, Y) in tqdm(enumerate(trainLoader)):\n",
    "    with torch.no_grad():\n",
    "        X = model.visual_encoder(X.to(device))\n",
    "        for j in range(len(X)):\n",
    "            if Y[j].item() not in mean_dict:\n",
    "                mean_dict[Y[j].item()] = X[j]\n",
    "                count_dict[Y[j].item()] = 1\n",
    "            else:\n",
    "                mean_dict[Y[j].item()] = mean_dict[Y[j].item()] * (count_dict[Y[j].item()] / (count_dict[Y[j].item()]+1)) + X[j] / (count_dict[Y[j].item()]+1)\n",
    "                count_dict[Y[j].item()] += 1\n",
    "    #             print(Y[j], mean_dict[Y[j].item()].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991463f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_embedding(model, image_embeds, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n",
    "    batch_size = image_embeds.shape[0]\n",
    "    if not sample:\n",
    "        image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)\n",
    "    image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image_embeds.device)\n",
    "    model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\n",
    "\n",
    "    prompt = [model.prompt] * batch_size\n",
    "    input_ids = model.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image_embeds.device) \n",
    "    input_ids[:,0] = model.tokenizer.bos_token_id\n",
    "    input_ids = input_ids[:, :-1] \n",
    "\n",
    "    if sample:\n",
    "        #nucleus sampling\n",
    "        outputs = model.text_decoder.generate(input_ids=input_ids,\n",
    "                                              max_length=max_length,\n",
    "                                              min_length=min_length,\n",
    "                                              do_sample=True,\n",
    "                                              top_p=top_p,\n",
    "                                              num_return_sequences=1,\n",
    "                                              eos_token_id=model.tokenizer.sep_token_id,\n",
    "                                              pad_token_id=model.tokenizer.pad_token_id, \n",
    "                                              repetition_penalty=1.1,                                            \n",
    "                                              **model_kwargs)\n",
    "    else:\n",
    "        #beam search\n",
    "        outputs = model.text_decoder.generate(input_ids=input_ids,\n",
    "                                              max_length=max_length,\n",
    "                                              min_length=min_length,\n",
    "                                              num_beams=num_beams,\n",
    "                                              eos_token_id=model.tokenizer.sep_token_id,\n",
    "                                              pad_token_id=model.tokenizer.pad_token_id,     \n",
    "                                              repetition_penalty=repetition_penalty,\n",
    "                                              **model_kwargs)            \n",
    "\n",
    "    captions = []    \n",
    "    for output in outputs:\n",
    "        caption = model.tokenizer.decode(output, skip_special_tokens=True)    \n",
    "        captions.append(caption[len(model.prompt):])\n",
    "    return captions\n",
    "\n",
    "\n",
    "# for k, v in mean_dict.items():\n",
    "#     with torch.no_grad():\n",
    "#         # beam search\n",
    "#         caption = generate_with_embedding(model, (v[None, ...]*np.random.rand()).to(device), sample=False, num_beams=3, max_length=20, min_length=5) \n",
    "#         # nucleus sampling\n",
    "#         # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \n",
    "#         print('caption: '+ caption[0], trainLoader.dataset.classes[k])\n",
    "# #         plt.imshow(v.permute(1,2,0).cpu().numpy())\n",
    "# #         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5059eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird - frog\n",
      "caption: a bird standing on a pole\n",
      "bird - deer\n",
      "caption: a woman in a blue dress\n",
      "bird - truck\n",
      "caption: a bird bird - bird - bird - bird - bird - bird - bird -\n",
      "bird - automobile\n",
      "caption: a bird bird bird - nature nature bird bird bird bird bird bird bird bird bird\n",
      "bird - cat\n",
      "caption: a bird in the sky\n",
      "bird - airplane\n",
      "caption: a black - crested crested crested crested crested crested crested crested crested crested crested crested crested\n",
      "bird - ship\n",
      "caption: a young - brown - crested - crested - crested - crested - crested - crested\n",
      "bird - horse\n",
      "caption: a bird's face\n",
      "bird - dog\n",
      "caption: a bird in the sky\n",
      "frog - bird\n",
      "caption: a man's face, with the skin and eyes\n",
      "frog - deer\n",
      "caption: a woman's face with a black and white background\n",
      "frog - truck\n",
      "caption: a close a a a a a a a a a a a a a a\n",
      "frog - automobile\n",
      "caption: a close a a a a a a a a a a a a a a\n",
      "frog - cat\n",
      "caption: a plant with a green background\n",
      "frog - airplane\n",
      "caption: a green leopard leopard leopard leopard leopard leopard leopard leopard leopard leopard leopard leopard leopard leopard\n",
      "frog - ship\n",
      "caption: a brown chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate\n",
      "frog - horse\n",
      "caption: a yellow, green, green, green, green, green, green, green\n",
      "frog - dog\n",
      "caption: an abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract\n",
      "deer - bird\n",
      "caption: a deer deer deer deer deer deer deer deer deer deer deer deer deer deer deer\n",
      "deer - frog\n",
      "caption: a horse in a field\n",
      "deer - truck\n",
      "caption: a deer nature nature nature nature nature nature nature nature nature nature nature nature nature nature\n",
      "deer - automobile\n",
      "caption: a close theninininininininininini nature nature\n",
      "deer - cat\n",
      "caption: a field with a view of a deer and a deer in the distance of a\n",
      "deer - airplane\n",
      "caption: a deer deer deer deer deer deer deer deer deer deer deer deer deer deer deer\n",
      "deer - ship\n",
      "caption: a young female deer deer\n",
      "deer - horse\n",
      "caption: a deer in the woods\n",
      "deer - dog\n",
      "caption: a forest in the forest\n",
      "truck - bird\n",
      "caption: the truck and truck truck truck truck truck truck truck truck truck truck truck truck truck\n",
      "truck - frog\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck\n",
      "truck - deer\n",
      "caption: a truck with a truck in the front of the truck\n",
      "truck - automobile\n",
      "caption: a large container with a large container on the side and a large container with a\n",
      "truck - cat\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck\n",
      "truck - airplane\n",
      "caption: a truck with a truck truck, truck truck, truck truck, truck truck,\n",
      "truck - ship\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck\n",
      "truck - horse\n",
      "caption: a truck, truck, truck, truck, truck, truck, truck, truck\n",
      "truck - dog\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck\n",
      "automobile - bird\n",
      "caption: the car car car car cars cars cars cars cars cars cars cars cars cars cars\n",
      "automobile - frog\n",
      "caption: the car in the car in the car in the car in the car in the\n",
      "automobile - deer\n",
      "caption: the car nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan\n",
      "automobile - truck\n",
      "caption: a woman in a car\n",
      "automobile - cat\n",
      "caption: the $ $ $ $ $ $ $ $ $ $ $ $ $ $ $\n",
      "automobile - airplane\n",
      "caption: a car car car cars cars cars cars cars cars cars cars cars cars cars cars\n",
      "automobile - ship\n",
      "caption: the car car car car car car car car car car car car car car car\n",
      "automobile - horse\n",
      "caption: the mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda\n",
      "automobile - dog\n",
      "caption: the $ $ $ $ $ $ $ $ $ $ $ $ $ $ $\n",
      "cat - bird\n",
      "caption: a cat in a room\n",
      "cat - frog\n",
      "caption: a cat sitting on a chair\n",
      "cat - deer\n",
      "caption: a woman with a cat in a cat\n",
      "cat - truck\n",
      "caption: a cat cat cat cat cat cat cat cat cat cat cat cat cat cat cat\n",
      "cat - automobile\n",
      "caption: aya the cat cat cat cat cat cat cat cat cat cat cat cat cat\n",
      "cat - airplane\n",
      "caption: a cat cat cat cat cat kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten\n",
      "cat - ship\n",
      "caption: a kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten\n",
      "cat - horse\n",
      "caption: a cat kitten kitten kitten kitten kitten kitten kitten kitten cat kitten kitten cat cat cat\n",
      "cat - dog\n",
      "caption: a man in a suit and tie\n",
      "airplane - bird\n",
      "caption: an airplane in the sky\n",
      "airplane - frog\n",
      "caption: an airplane in the sky sky sky blue sky sky blue sky sky sky sky sky\n",
      "airplane - deer\n",
      "caption: an airplane in the sky\n",
      "airplane - truck\n",
      "caption: a plane flying in the sky\n",
      "airplane - automobile\n",
      "caption: the sky glider glider glider glider glider glider glider glider the flight flight the flight the\n",
      "airplane - cat\n",
      "caption: the sky aero aero aero aero aero aero aero aero aero aero aero aero aero aero\n",
      "airplane - ship\n",
      "caption: a man in a black and white photo of a man in a black and white\n",
      "airplane - horse\n",
      "caption: a plane in the sky\n",
      "airplane - dog\n",
      "caption: the sky sky sky sky sky sky sky sky sky sky sky sky sky sky sky\n",
      "ship - bird\n",
      "caption: a boat in the water\n",
      "ship - frog\n",
      "caption: a boat in the water\n",
      "ship - deer\n",
      "caption: a boat in the water\n",
      "ship - truck\n",
      "caption: a boat in the sea\n",
      "ship - automobile\n",
      "caption: a sail sailing on the water\n",
      "ship - cat\n",
      "caption: the blue sea boats on the blue sea regatta boats on the blue lake boats regatta\n",
      "ship - airplane\n",
      "caption: a boat in the water\n",
      "ship - horse\n",
      "caption: a boat in the water\n",
      "ship - dog\n",
      "caption: the blue sea boats yacht yacht yacht yacht yacht yacht yacht yacht yacht yacht yacht yacht\n",
      "horse - bird\n",
      "caption: a horse and rider in a horse stables\n",
      "horse - frog\n",
      "caption: a horse horse horse horse horse horse horse horse horse horse horse horse horse horse horse\n",
      "horse - deer\n",
      "caption: a horse with a rider in the background\n",
      "horse - truck\n",
      "caption: a horse and a girl\n",
      "horse - automobile\n",
      "caption: a mare mare mare mare mare mare mare mare mare mare mare mare mare mare mare\n",
      "horse - cat\n",
      "caption: a horse and horse riding on a horse and horse riding on a horse and horse\n",
      "horse - airplane\n",
      "caption: a horse horse horse horse horse horse horse horse horse horse horse horse horse horse horse\n",
      "horse - ship\n",
      "caption: a horse horse horse horse horse horse horse horse horse horse horse horse horse horse horse\n",
      "horse - dog\n",
      "caption: a horse and a horse riding on the track\n",
      "dog - bird\n",
      "caption: a dog puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy\n",
      "dog - frog\n",
      "caption: a dog dog in a dog dog dog dog dog dog dog dog dog dog dog\n",
      "dog - deer\n",
      "caption: a dog in a white and a black and a white and a white and a\n",
      "dog - truck\n",
      "caption: a golden golden golden golden golden golden golden golden golden golden golden golden golden golden golden\n",
      "dog - automobile\n",
      "caption: aya the golden the golden golden golden golden golden golden golden golden golden golden the\n",
      "dog - cat\n",
      "caption: a dog and a dog on a leash\n",
      "dog - airplane\n",
      "caption: a puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy\n",
      "dog - ship\n",
      "caption: a puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy\n",
      "dog - horse\n",
      "caption: a dog puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy\n"
     ]
    }
   ],
   "source": [
    "for k1, v1 in mean_dict.items():\n",
    "    for k2, v2 in mean_dict.items():\n",
    "        if (k1==k2):\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            # beam search\n",
    "            caption = generate_with_embedding(model, (v1-v2)[None, ...].to(device), sample=False, num_beams=3, max_length=20, min_length=5) \n",
    "            # nucleus sampling\n",
    "            # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \n",
    "            print(f'{trainLoader.dataset.classes[k1]} - {trainLoader.dataset.classes[k2]}')\n",
    "            print('caption: '+ caption[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58efd569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# nc is the number of samples that have been seen\n",
    "# mu_x is the mean of the seen samples\n",
    "# new_data should be of shape (bsz, num_patches, embed_dim)\n",
    "# all data should belong to the same class\n",
    "def updateStat(nc, mu_x, cov_x, new_data):\n",
    "    lc = new_data.shape[0]\n",
    "    mu_y = new_data.mean(0)\n",
    "    \n",
    "    cov_y = torch.einsum('ijk, ijh->jkh', new_data-mu_y, new_data-mu_y)\n",
    "    cov_update = cov_x + cov_y + (nc**2*lc+lc**2*nc)/((nc+lc)**2)*(torch.einsum('ij,ik->ijk', mu_y-mu_x,mu_y-mu_x))\n",
    "    \n",
    "    return (mu_y*lc + mu_x*nc)/(nc+lc), cov_update, nc+lc\n",
    "\n",
    "\n",
    "# (num_patch, embed_dim) is the shape of the vision transformer output. Notice that \n",
    "# ViT chops images into patches, and applies transformation to each patch. \n",
    "# Usually num_patch = 1 + number of patches, and the first dimension is extracted\n",
    "# for classification task (such as CLIP does).\n",
    "# For base ViT, num_patch = 577, embed_dim=768\n",
    "# trainLoader contain sample from the same class\n",
    "def getStat(trainLoader, model, num_patch=577, embed_dim=768):\n",
    "    nc = 0\n",
    "    mu_x = torch.zeros(num_patch, embed_dim).to(device)\n",
    "    cov_x = torch.zeros(num_patch, embed_dim, embed_dim).to(device)\n",
    "    for i, (X, Y) in tqdm(enumerate(trainLoader)):\n",
    "        with torch.no_grad():\n",
    "            feature = model.visual_encoder(X.to(device))\n",
    "            mu_x, cov_x, nc = updateStat(nc, mu_x, cov_x, feature)\n",
    "    return mu_x, cov_x, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89367631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "num_classes = 10\n",
    "trainLoaders = [[] for i in range(num_classes)]\n",
    "\n",
    "for k in range(num_classes):\n",
    "    trainSubset = Subset(train_ds, (torch.tensor(train_ds.targets)==k).nonzero().squeeze()[0:2000])\n",
    "    trainLoaders[k] =torch.utils.data.DataLoader(\n",
    "        trainSubset,\n",
    "        batch_size=20,\n",
    "        shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a25ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "mu_x0, cov_x0, nc0 = getStat(trainLoaders[4], model, num_patch=577, embed_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bd7c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:28,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "mu_x1, cov_x1, nc1 = getStat(trainLoaders[5], model, num_patch=577, embed_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59afcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption: flight flight flight flight flight flight flight flight flight flight flight flight flight flight flight flight\n",
      "caption: car car car car car car car car car car car car car car car car\n",
      "caption: bird bird bird bird bird bird bird bird bird bird bird bird bird bird bird bird\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1686/1477317933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_with_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'caption: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1686/2855505991.py\u001b[0m in \u001b[0;36mgenerate_with_embedding\u001b[0;34m(model, image_embeds, sample, num_beams, max_length, min_length, top_p, repetition_penalty)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimage_atts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"encoder_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoder_attention_mask\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimage_atts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(num_classes):\n",
    "    trainLoader = trainLoaders[k]\n",
    "    for i, (x, y) in enumerate(trainLoader):\n",
    "        with torch.no_grad():\n",
    "            v = model.visual_encoder(x.to(device)).mean(0)\n",
    "            caption = generate_with_embedding(model, v[None, ...].to(device), sample=False, num_beams=3, max_length=20, min_length=5) \n",
    "            print('caption: '+ caption[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a690f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual_embeds = []\n",
    "# for k in range(num_classes):\n",
    "#     for i, (x, y) in enumerate(trainLoaders[k]):\n",
    "#         with torch.no_grad():\n",
    "#             if (len(visual_embeds)<=k):\n",
    "#                 visual_embeds.append([model.visual_encoder(x.to(device))])\n",
    "#             else:\n",
    "#                 visual_embeds[k].append(model.visual_encoder(x.to(device)))\n",
    "\n",
    "visual_embeds = []\n",
    "for k in range(num_classes):\n",
    "    x, y = next(iter(trainLoaders[k]))\n",
    "    with torch.no_grad():\n",
    "        visual_embeds.append(model.visual_encoder(x.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98e9b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption: a plane in the sky (0, 1)\n",
      "caption: an airplane in the sky (0, 2)\n",
      "caption: the sky aero aero aero aero aero aero aero aero aero aero aero aero aero aero (0, 3)\n",
      "caption: an airplane in the sky (0, 4)\n",
      "caption: the blue sky sky sky sky sky an an an an an an an an an (0, 5)\n",
      "caption: an airplane in the sky sky sky sky sky sky sky sky sky sky sky sky (0, 6)\n",
      "caption: a plane in the sky (0, 7)\n",
      "caption: a man in a black and white photo of a man in a black and white (0, 8)\n",
      "caption: a plane flying in the sky (0, 9)\n",
      "caption: a car'car'''''''''''' (1, 0)\n",
      "caption: the car stock car stock car stock cars cars cars cars cars cars cars cars cars (1, 2)\n",
      "caption: the $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ (1, 3)\n",
      "caption: the car nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan nissan (1, 4)\n",
      "caption: the car'car'car'car'car'car'car'car (1, 5)\n",
      "caption: the car in the car in the car in the car in the car in the (1, 6)\n",
      "caption: the mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda mazda (1, 7)\n",
      "caption: the car car car car car car car car car car car car car car car (1, 8)\n",
      "caption: a woman in a car (1, 9)\n",
      "caption: a black - crested crested crested crested crested crested crested crested crested crested crested crested crested (2, 0)\n",
      "caption: a close - bird bird bird bird - bird - bird - bird - bird - (2, 1)\n",
      "caption: a bird standing on a branch (2, 3)\n",
      "caption: a woman in a black dress (2, 4)\n",
      "caption: a bird in the sky (2, 5)\n",
      "caption: a bird standing on a pole (2, 6)\n",
      "caption: a bird's logo (2, 7)\n",
      "caption: a young - crested crested crested crested crested crested crested crested crested crested crested crested crested (2, 8)\n",
      "caption: a bird bird - bird - bird - bird - bird - bird - bird - (2, 9)\n",
      "caption: a cat kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten (3, 0)\n",
      "caption: aya the cat cat cat cat cat cat cat cat cat cat cat cat cat (3, 1)\n",
      "caption: a cat in a room (3, 2)\n",
      "caption: a woman with a cat (3, 4)\n",
      "caption: a man in a suit and tie (3, 5)\n",
      "caption: a cat sitting on a chair (3, 6)\n",
      "caption: a cat kitten kitten kitten kitten kitten kitten cat cat cat cat cat cat cat cat (3, 7)\n",
      "caption: a kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten kitten (3, 8)\n",
      "caption: a cat cat cat cat cat cat cat cat cat cat cat cat cat cat cat (3, 9)\n",
      "caption: a deer in the woods (4, 0)\n",
      "caption: a bird andninininininini nature nature nature nature nature f (4, 1)\n",
      "caption: a horse in a field (4, 2)\n",
      "caption: a field with a view of a deer and a deer in the distance of the (4, 3)\n",
      "caption: a field in the distance (4, 5)\n",
      "caption: a horse in a field (4, 6)\n",
      "caption: a deer in the woods (4, 7)\n",
      "caption: a young female black squirrel (4, 8)\n",
      "caption: a deer and a nature nature nature nature nature nature nature nature (4, 9)\n",
      "caption: a puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy (5, 0)\n",
      "caption: aya the golden the golden the golden golden golden golden golden golden small small small (5, 1)\n",
      "caption: a dog with a dog puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy (5, 2)\n",
      "caption: a dog with a dog on it's back (5, 3)\n",
      "caption: a dog in a white and a black and a white and a white and a (5, 4)\n",
      "caption: a dog dog in a dog dog dog dog dog dog dog dog dog dog dog (5, 6)\n",
      "caption: a small dog puppy dog, a small, small, small, small, small (5, 7)\n",
      "caption: a puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy puppy (5, 8)\n",
      "caption: a golden golden golden golden golden golden golden golden golden golden golden golden golden golden golden (5, 9)\n",
      "caption: a chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate (6, 0)\n",
      "caption: a closey a - a - a - a - a - a - a (6, 1)\n",
      "caption: a man's face in a black and white background (6, 2)\n",
      "caption: a plant with a green background (6, 3)\n",
      "caption: a woman's face with a black and white background (6, 4)\n",
      "caption: an abstract, abstract, abstract, abstract, abstract, abstract, abstract, abstract (6, 5)\n",
      "caption: a yellow, green, green, green, green, green, green, green (6, 7)\n",
      "caption: a brown chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate chocolate (6, 8)\n",
      "caption: a green green green green mouse mouse mouse - a - a - a - a (6, 9)\n",
      "caption: a horse horse horse horse horse horse horse horse horse horse horse horse horse horse horse (7, 0)\n",
      "caption: a mare mare mare mare mare mare mare mare mare mare mare mare mare mare mare (7, 1)\n",
      "caption: a horse and rider in a horse stables (7, 2)\n",
      "caption: a horse and horse riding on a horse horse riding horse riding horse riding horse horse (7, 3)\n",
      "caption: a horse with a rider in the background (7, 4)\n",
      "caption: a horse and a horse riding on a horse riding (7, 5)\n",
      "caption: a horse horse horse horse horse horse horse horse horse horse horse horse horse horse horse (7, 6)\n",
      "caption: a horse horse horse horse horse horse horse horse horse horse horse horse horse horse horse (7, 8)\n",
      "caption: a horse and a horse and a horse and a horse (7, 9)\n",
      "caption: a boat in the water (8, 0)\n",
      "caption: a sail sailing in the water (8, 1)\n",
      "caption: a boat in the water (8, 2)\n",
      "caption: the blue sky boats on the lake boats regatta on the lake boats regatta on the (8, 3)\n",
      "caption: a boat in the water (8, 4)\n",
      "caption: the blue sky boats yacht yacht yacht yacht yacht yacht yacht yacht yacht yacht yacht yacht (8, 5)\n",
      "caption: a boat on the water in the water (8, 6)\n",
      "caption: a boat in the water (8, 7)\n",
      "caption: a boat in the sea (8, 9)\n",
      "caption: a truck with a truck and a truck in the background (9, 0)\n",
      "caption: a large container with a large container on the side and a large container with a (9, 1)\n",
      "caption: the truck and truck trucks in the truck and truck trucks, trucks, trucks, (9, 2)\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck (9, 3)\n",
      "caption: a truck with a truck in the front of the truck (9, 4)\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck (9, 5)\n",
      "caption: the truck truck truck truck truck truck truck truck truck truck truck truck truck truck truck (9, 6)\n",
      "caption: a truck, truck, truck, truck, truck, truck, truck, truck (9, 7)\n",
      "caption: a truck with a truck truck truck truck truck truck truck truck truck truck truck truck (9, 8)\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        if k==j:\n",
    "            continue\n",
    "        v1 = visual_embeds[k].mean(0)\n",
    "        v2 = visual_embeds[j].mean(0)\n",
    "        caption = generate_with_embedding(model, (v1-v2)[None, ...].to(device), sample=False, num_beams=3, max_length=20, min_length=5) \n",
    "        print('caption: '+ caption[0], (k, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7fd41ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([577, 768]) torch.Size([100, 577, 768]) torch.Size([100, 577, 768]) torch.Size([577, 768])\n"
     ]
    }
   ],
   "source": [
    "v0 = visual_embeds[2]\n",
    "v1 = visual_embeds[3]\n",
    "\n",
    "mu0 = v0.mean(0)\n",
    "mu1 = v1.mean(0)\n",
    "\n",
    "print(mu0.shape, v0.shape, v1.shape, mu1.shape)\n",
    "\n",
    "v0_ = v0 - mu0\n",
    "v1_ = v1 - mu1\n",
    "\n",
    "Sigma0 = torch.einsum('ijk,ijh->jkh', v0_, v0_) / (v0_.shape[0]-1)\n",
    "Sigma1 = torch.einsum('ijk,ijh->jkh', v1_, v1_) / (v1_.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d65bafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7152/3869384989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m577\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     w.append(torch.linalg.inv((1-lam)*(cov_x0[i]+cov_x1[i]) + lam * torch.eye(cov_x0[i].shape[1]).to(device))@(mu_x1[i]-mu_x0[i]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_x0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcov_x1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_x0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmu_x1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu_x0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "lam = 0\n",
    "for i in tqdm(range(577)):\n",
    "#     w.append(torch.linalg.inv((1-lam)*(cov_x0[i]+cov_x1[i]) + lam * torch.eye(cov_x0[i].shape[1]).to(device))@(mu_x1[i]-mu_x0[i]))\n",
    "    w.append(torch.linalg.solve((1-lam)*(cov_x0[i]+cov_x1[i]) + lam * torch.eye(cov_x0[i].shape[1]).to(device), (mu_x1[i]-mu_x0[i])))\n",
    "h = torch.stack(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c87260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(764, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cov_x0[3]+cov_x1[3]\n",
    "b = torch.linalg.inv(a)\n",
    "torch.linalg.matrix_rank(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02129e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption: a person standing in front of a building\n"
     ]
    }
   ],
   "source": [
    "caption = generate_with_embedding(model, (h[None, ...]).to(device), sample=False, num_beams=10, max_length=20, min_length=5) \n",
    "print('caption: '+ caption[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d21fbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(2,5)\n",
    "b = a.T@a\n",
    "b.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
